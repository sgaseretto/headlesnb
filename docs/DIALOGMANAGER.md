# DialogManager Guide

DialogManager provides AI-assisted dialog conversations stored as Jupyter notebooks with extended metadata. This guide covers the architecture, concepts, and advanced usage patterns.

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Core Concepts](#core-concepts)
- [Message Types](#message-types)
- [LLM Integration](#llm-integration)
- [Context Building](#context-building)
- [Serialization](#serialization)
- [Undo/Redo System](#undoredo-system)
- [Advanced Patterns](#advanced-patterns)
- [Testing](#testing)

---

## Overview

DialogManager extends the concept of Jupyter notebooks to support AI-assisted dialogs. Instead of cells, dialogs contain **messages** with rich metadata for LLM interactions.

Key features:
- Message-based conversation management
- LLM integration with context building
- Bidirectional serialization (Dialog <-> Notebook)
- Full undo/redo support
- Pinned/skipped message control

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DialogManager                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Dialogs    â”‚  â”‚   Messages   â”‚  â”‚     LLM Integration     â”‚ â”‚
â”‚  â”‚             â”‚  â”‚              â”‚  â”‚                         â”‚ â”‚
â”‚  â”‚ - DialogInfoâ”‚  â”‚ - Content    â”‚  â”‚ - LLMClient interface   â”‚ â”‚
â”‚  â”‚ - Path      â”‚  â”‚ - Type       â”‚  â”‚ - MockLLMClient         â”‚ â”‚
â”‚  â”‚ - Shell     â”‚  â”‚ - Output     â”‚  â”‚ - ContextBuilder        â”‚ â”‚
â”‚  â”‚ - Messages  â”‚  â”‚ - Metadata   â”‚  â”‚ - Token budgets         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                â”‚                       â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                          â”‚                                       â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                 â”‚  Serialization  â”‚                              â”‚
â”‚                 â”‚                 â”‚                              â”‚
â”‚                 â”‚ Dialog <-> .ipynb                              â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                          â”‚                                       â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                 â”‚    execnb       â”‚                              â”‚
â”‚                 â”‚  (CaptureShell) â”‚                              â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

1. **DialogManager**: Main coordinator class
2. **DialogInfo**: Stores dialog state and metadata
3. **Message**: Individual message dataclass
4. **Serialization**: Dialog <-> Notebook conversion
5. **LLM Client**: Abstract interface for LLM calls
6. **ContextBuilder**: Smart context window management

---

## Core Concepts

### Dialogs

A dialog is a conversation stored as a Jupyter notebook. Each dialog has:
- A unique name (identifier)
- A file path (.ipynb)
- A kernel (CaptureShell) for code execution
- A list of messages
- An optional LLM client

```python
from headlesnb import DialogManager
from headlesnb.dialogmanager.llm import MockLLMClient

manager = DialogManager(
    root_path=".",
    default_llm_client=MockLLMClient(responses=["Hello!"])
)

# Create a new dialog
manager.use_dialog("chat", "chat.ipynb", mode="create")

# Connect to existing dialog
manager.use_dialog("existing", "existing.ipynb", mode="connect")
```

### Messages

Messages are the building blocks of dialogs. Each message has:

```python
from headlesnb.dialogmanager import Message

message = Message(
    content="import pandas as pd",
    msg_type="code",        # 'code', 'note', 'prompt', 'raw'
    output="",              # Execution output or LLM response
    pinned=0,               # 1 = always include in LLM context
    skipped=0,              # 1 = exclude from LLM context
    time_run=None,          # Timestamp of last execution
    is_exported=0,          # Export flag
    i_collapsed=0,          # Input collapsed state
    o_collapsed=0,          # Output collapsed state
    heading_collapsed=0,    # Heading collapsed state
    use_thinking=False      # Enable thinking mode for LLM
)
```

---

## Message Types

### Code Messages

Executable Python code. When included in LLM context, code is wrapped in code blocks with outputs.

```python
manager.add_message("import pandas as pd\ndf = pd.read_csv('data.csv')", msg_type='code')

# Execute the code
outputs = manager.execute_code(code="print(df.head())")
```

In LLM context:
```
```python
import pandas as pd
df = pd.read_csv('data.csv')
```

Output:
```
   col1  col2
0     1     4
1     2     5
```
```

### Note Messages

Markdown documentation and context. Useful for providing background information.

```python
manager.add_message("""
# Data Analysis Project

This notebook analyzes sales data from Q4 2024.
Key metrics:
- Revenue by region
- Customer acquisition cost
- Churn rate
""", msg_type='note')
```

In LLM context:
```
[Note]
# Data Analysis Project

This notebook analyzes sales data from Q4 2024.
...
```

### Prompt Messages

Questions for the LLM. The content is sent as a user message, and the response is stored as output.

```python
manager.add_message("Explain what the code above does", msg_type='prompt')
response = manager.execute_prompt()
print(response.content)  # LLM's explanation
```

### Raw Messages

Unprocessed content. Included as-is in the LLM context.

```python
manager.add_message("Some raw content", msg_type='raw')
```

---

## LLM Integration

### LLMClient Interface

Implement this interface to integrate your LLM:

```python
from headlesnb.dialogmanager.llm import LLMClient, LLMResponse
from typing import List, Dict, Any

class MyLLMClient(LLMClient):
    def chat(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: str = "",
        **kwargs
    ) -> LLMResponse:
        # Call your LLM API
        response = my_api.chat(messages, system=system_prompt)

        return LLMResponse(
            content=response.text,
            tool_calls=response.tool_calls,
            usage={"input_tokens": 100, "output_tokens": 50},
            model="my-model",
            stop_reason="end_turn"
        )

    def count_tokens(self, text: str) -> int:
        # Use your tokenizer
        return len(my_tokenizer.encode(text))
```

### LLMResponse

Standard response format:

```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

@dataclass
class LLMResponse:
    content: str                              # Response text
    tool_calls: Optional[List[Dict]] = None   # Tool/function calls
    usage: Optional[Dict[str, int]] = None    # Token usage
    model: Optional[str] = None               # Model used
    stop_reason: Optional[str] = None         # Why stopped
```

### MockLLMClient

For testing without API calls:

```python
from headlesnb.dialogmanager.llm import MockLLMClient, MockLLMResponse

# Simple responses
client = MockLLMClient(responses=[
    "First response",
    "Second response"
])

# With tool calls
client = MockLLMClient(responses=[
    MockLLMResponse(
        content="I'll search for that",
        tool_calls=[{
            "name": "search",
            "input": {"query": "test"}
        }]
    ),
    "Found 5 results"
])

# Helper for tool use testing
from headlesnb.dialogmanager.llm import create_mock_for_tool_use

client = create_mock_for_tool_use(
    tool_name="calculator",
    tool_input={"expression": "2+2"},
    final_response="The result is 4"
)
```

---

## Context Building

ContextBuilder creates LLM context from dialog messages with token budget management.

### Basic Usage

```python
from headlesnb.dialogmanager.llm import ContextBuilder

builder = ContextBuilder(
    llm_client=my_client,  # For token counting
    max_tokens=200000      # Context window size
)

messages = builder.build_context(
    dialog_messages=dialog.messages,
    current_prompt="What does this code do?",
    include_outputs=True,
    system_prompt="You are a helpful assistant.",
    reserved_tokens=4096  # For response
)
```

### Message Filtering

**Pinned Messages**: Always included, regardless of token budget.

```python
# Pin important context
manager.add_message("API_KEY = 'xxx'", msg_type='code', pinned=1)
```

**Skipped Messages**: Never included.

```python
# Skip old, irrelevant content
manager.update_message(msg_id, skipped=1)
```

### Token Budget Strategy

1. Reserve tokens for response
2. Account for system prompt
3. Always include pinned messages
4. Skip messages marked as skipped
5. Include recent messages newest-first until budget exhausted
6. Maintain chronological order in final output

---

## Serialization

Dialogs are stored as Jupyter notebooks with extended cell metadata.

### Dialog to Notebook

```python
from headlesnb.dialogmanager import dialog_to_notebook

notebook = dialog_to_notebook(dialog_info)
# Returns nbformat.NotebookNode
```

### Notebook to Dialog

```python
from headlesnb.dialogmanager import notebook_to_dialog

messages = notebook_to_dialog(notebook)
# Returns List[Message]
```

### Cell Format

Each message becomes a cell with metadata:

```json
{
  "cell_type": "code",
  "metadata": {
    "solveit": {
      "msg_type": "code",
      "msg_id": "abc123",
      "pinned": 0,
      "skipped": 0,
      "time_run": "2024-01-15T10:30:00"
    }
  },
  "source": ["import pandas as pd"],
  "outputs": []
}
```

### Prompt Separator Pattern

Prompt messages use a special separator to store both content and LLM response:

```markdown
What does this code do?

##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_abc123def456 -->

The code imports pandas and creates a DataFrame...
```

---

## Undo/Redo System

DialogManager uses the Command Pattern for full undo/redo support.

### Supported Operations

- `InsertMessageCommand` - Adding messages
- `DeleteMessageCommand` - Removing messages
- `UpdateMessageCommand` - Modifying message content/attributes
- `MoveMessageCommand` - Repositioning messages
- `SwapMessagesCommand` - Swapping two messages
- `UpdateMessageOutputCommand` - Updating outputs (for prompt execution)

### Usage

```python
# Perform operations
msg1 = manager.add_message("First", msg_type='note')
msg2 = manager.add_message("Second", msg_type='note')
manager.swap_messages(msg1, msg2)

# Undo
manager.undo()  # Undo swap
manager.undo(steps=2)  # Undo both adds

# Redo
manager.redo()  # Redo first add
manager.redo(steps=2)  # Redo add and swap

# View history
history = manager.get_history()
print(history['undo_stack'])  # Operations available to undo
print(history['redo_stack'])  # Operations available to redo

# Clear history
manager.clear_history()
```

### What's NOT Tracked

- `execute_code` - Code execution doesn't modify dialog structure
- `execute_prompt` output - Stored separately, can be re-run
- Read operations - Non-destructive queries

---

## Advanced Patterns

### Pattern 1: Iterative Analysis

Build analysis incrementally with LLM assistance:

```python
manager.use_dialog("analysis", "analysis.ipynb", mode="create")

# Set up context
manager.add_message("# Sales Analysis\nQ4 2024 data exploration", msg_type='note', pinned=1)
manager.add_message("import pandas as pd\ndf = pd.read_csv('sales.csv')", msg_type='code')
manager.execute_code(code="import pandas as pd\ndf = pd.read_csv('sales.csv')")

# First question
manager.add_message("What patterns do you see in this data?", msg_type='prompt')
response1 = manager.execute_prompt()

# Follow-up based on response
manager.add_message("Show me code to visualize the trend you mentioned", msg_type='prompt')
response2 = manager.execute_prompt()
```

### Pattern 2: Code Review Dialog

```python
manager.use_dialog("review", "review.ipynb", mode="create")

# Add code to review
manager.add_message('''
def calculate_metrics(data):
    results = []
    for item in data:
        if item['value'] > 0:
            results.append(item['value'] * 1.1)
    return results
''', msg_type='code')

# Request review
manager.add_message("Review this code for performance and suggest improvements", msg_type='prompt')
response = manager.execute_prompt(
    system_prompt="You are an expert Python code reviewer."
)
```

### Pattern 3: Multi-Step Problem Solving

```python
manager.use_dialog("problem", "problem.ipynb", mode="create")

# Define the problem
manager.add_message("""
# Problem: Optimize Database Queries

Current issue:
- API response time: 2.5s average
- Database queries: 15 per request
- N+1 query pattern detected
""", msg_type='note', pinned=1)

# Step 1: Analysis
manager.add_message("Analyze the root causes of this performance issue", msg_type='prompt')
analysis = manager.execute_prompt()

# Step 2: Solution
manager.add_message("Propose a solution with code examples", msg_type='prompt')
solution = manager.execute_prompt()

# Step 3: Implementation
manager.add_message("Write the implementation code", msg_type='prompt')
implementation = manager.execute_prompt()
```

### Pattern 4: Context-Aware Debugging

```python
manager.use_dialog("debug", "debug.ipynb", mode="create")

# Pin the error context
manager.add_message("""
Error traceback:
```
TypeError: 'NoneType' object is not subscriptable
  File "app.py", line 45, in process_data
    result = data['key']['nested']
```
""", msg_type='note', pinned=1)

# Add relevant code
manager.add_message('''
def process_data(data):
    result = data['key']['nested']
    return result
''', msg_type='code')

# Debug
manager.add_message("What's causing this error and how do I fix it?", msg_type='prompt')
response = manager.execute_prompt()
```

### Pattern 5: Tool Use with Mock

```python
from headlesnb.dialogmanager.llm import create_mock_for_tool_use

# Simulate tool-using LLM
client = create_mock_for_tool_use(
    tool_name="search_codebase",
    tool_input={"query": "authentication"},
    final_response="Found 3 relevant files: auth.py, login.py, session.py"
)

manager = DialogManager(default_llm_client=client)
manager.use_dialog("search", "search.ipynb", mode="create")

manager.add_message("Find all authentication-related code", msg_type='prompt')
response = manager.execute_prompt()

# First response has tool call
if response.tool_calls:
    tool_call = response.tool_calls[0]
    # Handle tool execution...

    # Continue conversation
    manager.add_message("Now summarize what you found", msg_type='prompt')
    final_response = manager.execute_prompt()
```

---

## Testing

### Testing with MockLLMClient

```python
import pytest
from headlesnb import DialogManager
from headlesnb.dialogmanager.llm import MockLLMClient

def test_dialog_flow():
    client = MockLLMClient(responses=[
        "I understand you want to analyze data.",
        "Here's the analysis: ..."
    ])

    manager = DialogManager(default_llm_client=client)
    manager.use_dialog("test", "test.ipynb", mode="create")

    manager.add_message("Analyze this data", msg_type='prompt')
    response1 = manager.execute_prompt()
    assert "analyze" in response1.content.lower()

    manager.add_message("Continue", msg_type='prompt')
    response2 = manager.execute_prompt()
    assert "analysis" in response2.content.lower()
```

### Testing Undo/Redo

```python
def test_undo_redo():
    manager = DialogManager()
    manager.use_dialog("test", "test.ipynb", mode="create")

    # Add messages
    msg1 = manager.add_message("First", msg_type='note')
    msg2 = manager.add_message("Second", msg_type='note')

    assert len(manager.list_messages()) == 2

    # Undo
    manager.undo()
    assert len(manager.list_messages()) == 1

    # Redo
    manager.redo()
    assert len(manager.list_messages()) == 2
```

### Testing Serialization

```python
from headlesnb.dialogmanager import (
    dialog_to_notebook,
    notebook_to_dialog,
    Message
)

def test_roundtrip():
    messages = [
        Message(content="import pandas", msg_type='code'),
        Message(content="What's next?", msg_type='prompt', output="Try this...")
    ]

    # Create dialog info
    dialog = DialogInfo(name="test", messages=messages)

    # Convert to notebook
    notebook = dialog_to_notebook(dialog)

    # Convert back
    recovered = notebook_to_dialog(notebook)

    assert len(recovered) == 2
    assert recovered[0].msg_type == 'code'
    assert recovered[1].msg_type == 'prompt'
    assert recovered[1].output == "Try this..."
```

---

## Best Practices

1. **Pin Important Context**: Use `pinned=1` for information that should always be in LLM context.

2. **Skip Stale Content**: Mark old, irrelevant messages as `skipped=1` to save context space.

3. **Use Note Messages for Structure**: Add note messages as section headers and context.

4. **Test with MockLLMClient**: Develop and test without API calls.

5. **Monitor Token Usage**: Use `count_tokens()` and `ContextBuilder` to stay within limits.

6. **Leverage Undo/Redo**: Don't be afraid to experiment - you can always undo.

7. **Serialize Regularly**: Call `unuse_dialog()` to save to disk.

---

## See Also

- [API Reference](API.md) - Complete API documentation
- [Quick Start](../QUICKSTART.md) - Getting started guide
- [Implementation Summary](../IMPLEMENTATION_SUMMARY.md) - Architecture details
